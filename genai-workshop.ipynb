{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QC8yC7SoSMn"
   },
   "source": [
    "# **Apigee GenAI Workshop**\n",
    "\n",
    "<!-- <table align=\"left\">\n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/apigee-samples/blob/genai-workshop/genai-workshop.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\\\"><br> Open in Colab\n",
    "      </a>\n",
    "    </td>\n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapigee-samples%2Fgenai-workshop%2Fgenai-workshop.ipynb\">\n",
    "        <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "      </a>\n",
    "    </td>    \n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/apigee-samples/genai-workshop/genai-workshop.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "      </a>\n",
    "    </td>\n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://github.com/GoogleCloudPlatform/apigee-samples/blob/genai-workshop/genai-workshop.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "      </a>\n",
    "    </td>\n",
    "</table> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to Google's Apigee GenAI Workshop! \n",
    "\n",
    "This hands-on workshop will equip you with the knowledge and skills to leverage the power of Generative AI within your API ecosystem. Through practical exercises and real-world examples, you'll learn how to seamlessly integrate Large Language Models (LLMs) with Apigee, Google's leading API management platform. Get ready to unlock new possibilities and explore the exciting world of GenAI and APIs!\n",
    "\n",
    "You should already have a lab instance up and running with all the necessary artifacts (Apigee, Vertex AI, Vertex DB, etc) provisioned for you to use. \n",
    "\n",
    "First, lets install the necessary dependencies to run the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AuXsoJDZPMs"
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install langchain_google_vertexai\n",
    "!pip install langchain-openai\n",
    "!pip install google-cloud-aiplatform\n",
    "!pip install google-cloud-tasks\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4eVYn8frc5i"
   },
   "source": [
    "## Initialize notebook variables\n",
    "\n",
    "You can fetch all the variables from your lab instance\n",
    "\n",
    "* **PROJECT_ID**: The default GCP project provisioned\n",
    "* **LOCATION**: The default GCP Region where the project is provisioned.\n",
    "* **APIGEE_HOSTNAME**:  The hostname of the Apigee instance provisioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kj-10KmnZSVe"
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"\"  # @param {type:\"string\"}\n",
    "APIGEE_HOSTNAME = \"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Logging with Apigee\n",
    "\n",
    "Logging both prompts and candidate responses from large language models (LLMs) allows for detailed analysis and improvement of the model's performance over time. By examining past interactions, AI practitioners can identify patterns leading to refinements in the training data or model architecture. Furthermore, by examining the prompts security teams can detect malicious intent, such as attempts to extract sensitive information or generate harmful content.\n",
    "\n",
    "Additionally, logging the generated candidates provides insights into the LLM's behavior and helps identify any biases or vulnerabilities in the model itself. This information can then be used to improve security measures, fine-tune the model, and mitigate potential risks associated with LLM usage.\n",
    "\n",
    "![image](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-logging/images/llm-logging.png?raw=1)\n",
    "\n",
    "\n",
    "### Benefits of Logging with Apigee and Google Cloud Logging\n",
    "\n",
    "* **Seamless logging**: Effortlessly capture prompts, candidate responses, and metadata without complex coding.\n",
    "* **Scalable and secure**: Leverage Google Cloud's infrastructure for reliable and secure log management.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Prompt request is receved by an Apigee Proxy.\n",
    "2. Apigee extracts prompt and candidate responses.\n",
    "3. Apigee logs prompt and candidate responses to Cloud Logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S478piq-SGLc"
   },
   "source": [
    "### Test Sample\n",
    "\n",
    "Apigee allows you to seamlessly send logs to Cloud Logging using native integration with the [Message Logging](https://cloud.google.com/apigee/docs/api-platform/reference/policies/message-logging-policy#cloudloggingelement) policy. This sample also includes a message chunking solution that allows logging very long messages (ex. 1M tokens supported by Gemini) and connecting them together using a unique message identifier.\n",
    "\n",
    "With the following cell you'll be able to invoke an LLM and both prompt and candidate resposne will be logged in Cloud Logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87i610BzZns_"
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "if not LOCATION or LOCATION == \"\":\n",
    "    raise ValueError(\"Please set your LOCATION\")\n",
    "if not APIGEE_HOSTNAME or APIGEE_HOSTNAME == \"\":\n",
    "    raise ValueError(\"Please set your APIGEE_HOSTNAME\")\n",
    "\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-logging\"\n",
    "\n",
    "# Initialize Langchain\n",
    "model = VertexAI(\n",
    "      project=PROJECT_ID,\n",
    "      location=LOCATION,\n",
    "      api_endpoint=API_ENDPOINT,\n",
    "      api_transport=\"rest\",\n",
    "      streaming=True,\n",
    "      model_name=\"gemini-1.5-pro\")\n",
    "\n",
    "prompts = [\"Provide an explanation about the benefits of using sunscreen. Make sure to make it as long as a novel.\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "  print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjW8UBEnZg9U"
   },
   "source": [
    "### Explore and Analyze Logs with Cloud Logging\n",
    "\n",
    "1. Navigate to the Cloud Logging Explorer by [**clicking here**](https://console.cloud.google.com/logs/query?_ga=2.194228271.307340908.1727018794-898542846.1726863667).\n",
    "\n",
    "2. Set the query filter. Make sure to replace the `PROJECT_ID` with the Apigee project ID:\n",
    "\n",
    "  ```\n",
    "  logName=\"projects/PROJECT_ID/logs/apigee\"\n",
    "  ```\n",
    "3. Run the query and explore the logs. See example below:\n",
    "\n",
    "  ![image](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-logging/images/logs-explorer.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully deployed the llm-logging proxy and tested the ability to log the request and responses from subsequent LLM's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Routing with Apigee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a sample Apigee proxy to demonstrate the routing capabilities of Apigee across different LLM providers. In this sample we will use Google VertexAI, Mistral and HuggingFace as the LLM providers\n",
    "- The framework will easily help onboarding other providers using configurations\n",
    "\n",
    "![architecture](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-routing/images/arch.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "This lab requires you to have a HuggingFace Access Token and Mistral AI API Key. The provisioned lab instance **does not** include both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To create a HuggingFace Access Token:\n",
    "- You wll need to create an account in [HuggingFace](https://huggingface.co)\n",
    "- Go to settings, click `Access Tokens` and then click the \"Create new token\" button\n",
    "- Choose `Read` for token type, provide a name and then hit \"Create token\"\n",
    "- Copy the token below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Mistral AI API Key:\n",
    "- You wll need to create an account in [HuggingFace](https://mistral.ai )\n",
    "- Click `API Keys` and then click the \"Create new key\" button\n",
    "- Provide a name and then hit \"Create key\"\n",
    "- Copy the key below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRALAI_KEY=\"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Routing with Apigee:\n",
    "\n",
    "* **Configuration Driven Routing**: All the routing logic are driven through configuration which makes onboarding very easy\n",
    "* **Security**: Irrespective of the model and providers, Apigee will secure the endpoints\n",
    "* **Consistency**: Apigee can offer that layer of consistency to work with any LLM SDKs that are being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update HuggingFace and Mistral AI credentials in Apigee KVM Store\n",
    "\n",
    "We need to update the HuggingFace and Mistral AI credentials in the Apigee KVM store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "access_token = !gcloud auth print-access-token\n",
    "\n",
    "url = 'https://apigee.googleapis.com/v1/organizations/'+PROJECT_ID+'/environments/eval/keyvaluemaps/llm-routing-v1-modelprovider-config/entries'\n",
    "headers = {'Authorization': 'Bearer '+access_token[0], 'Content-type': 'application/json'}\n",
    "\n",
    "entry = 'huggingface__token'\n",
    "resp = requests.put(url+\"/\"+entry, headers = headers, data=json.dumps({\"name\": entry,\"value\": HUGGINGFACE_TOKEN}))\n",
    "if resp.status_code == 200:\n",
    "    print(\"HuggingFace Access Token updated successfully\")\n",
    "else:\n",
    "    print (resp.text)\n",
    "\n",
    "entry = 'mistral__token'\n",
    "resp = requests.put(url+\"/\"+entry, headers = headers, data=json.dumps({\"name\": entry,\"value\": MISTRALAI_KEY}))\n",
    "if resp.status_code == 200:\n",
    "    print(\"Mistral AI API Key updated successfully\")\n",
    "else:\n",
    "    print (resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample\n",
    "\n",
    "We will need an API Key that is already provisioned in your lab instance. Go to Apigee in your GCP console. In the left hand menu, select `Apps` to see the list of Apps. Click the `llm-routing-app` app and copy the `Key` from the `Credentials` section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTING_SAMPLE_APIKEY=\"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "if not LOCATION or LOCATION == \"\":\n",
    "    raise ValueError(\"Please set your LOCATION\")\n",
    "if not APIGEE_HOSTNAME or APIGEE_HOSTNAME == \"\":\n",
    "    raise ValueError(\"Please set your APIGEE_HOSTNAME\")\n",
    "if not ROUTING_SAMPLE_APIKEY or ROUTING_SAMPLE_APIKEY == \"\":\n",
    "    raise ValueError(\"Please set your ROUTING_SAMPLE_APIKEY\")\n",
    "\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-routing/\"\n",
    "\n",
    "PROMPT=\"Suggest name for a flower shop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an LLM Provider\n",
    "\n",
    "Select a provider from the dropdown. This will automatically set the model name used by the SDKs\n",
    "\n",
    "Try picking different providers from the dropdown above. You will see that the same SDK is able to call the Apigee endpoint serving responses from different providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from google.colab import auth\n",
    "\n",
    "llm_provider = \"select\" # @param [\"select\",\"google\", \"huggingface\", \"mistral\"]\n",
    "\n",
    "if llm_provider == \"google\":\n",
    "    model = \"google/gemini-1.5-flash\"\n",
    "elif llm_provider == \"mistral\":\n",
    "    model = \"open-mistral-nemo\"\n",
    "elif llm_provider == \"huggingface\":\n",
    "    model = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid LLM provider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = ROUTING_SAMPLE_APIKEY\n",
    "openai.base_url = API_ENDPOINT\n",
    "openai.default_headers = {\"x-apikey\": ROUTING_SAMPLE_APIKEY, \"x-llm-provider\": llm_provider}\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": PROMPT\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(f\"Using the OpenAI SDK, fetching the response from \\\"{model}\\\" provided by \\\"{llm_provider}\\\"\")\n",
    "print(\"\\n\")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=model,\n",
    "    api_key=ROUTING_SAMPLE_APIKEY,\n",
    "    base_url=API_ENDPOINT,\n",
    "    default_headers = {\"x-apikey\": ROUTING_SAMPLE_APIKEY, \"x-llm-provider\": llm_provider}\n",
    ")\n",
    "messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": PROMPT\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "print(f\"Using the Langchain SDK, fetching the response from \\\"{model}\\\" provided by \\\"{llm_provider}\\\"\")\n",
    "print(\"\\n\")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully deployed the routing proxy and tested the ability to route calls to different LLM providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Token-Limits with Apigee"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
