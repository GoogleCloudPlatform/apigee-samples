{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QC8yC7SoSMn"
   },
   "source": [
    "# **Apigee GenAI Workshop**\n",
    "\n",
    "<!-- <table align=\"left\">\n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/apigee-samples/blob/genai-workshop/genai-workshop.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\\\"><br> Open in Colab\n",
    "      </a>\n",
    "    </td>\n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapigee-samples%2Fgenai-workshop%2Fgenai-workshop.ipynb\">\n",
    "        <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "      </a>\n",
    "    </td>    \n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/apigee-samples/genai-workshop/genai-workshop.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "      </a>\n",
    "    </td>\n",
    "    <td style=\"text-align: center\">\n",
    "      <a href=\"https://github.com/GoogleCloudPlatform/apigee-samples/blob/genai-workshop/genai-workshop.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "      </a>\n",
    "    </td>\n",
    "</table> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to Google's Apigee GenAI Workshop! \n",
    "\n",
    "This hands-on workshop will equip you with the knowledge and skills to leverage the power of Generative AI within your API ecosystem. Through practical exercises and real-world examples, you'll learn how to seamlessly integrate Large Language Models (LLMs) with Apigee, Google's leading API management platform. Get ready to unlock new possibilities and explore the exciting world of GenAI and APIs!\n",
    "\n",
    "You should already have a lab instance up and running with all the necessary artifacts (Apigee, Vertex AI, Vertex DB, etc) provisioned for you to use. \n",
    "\n",
    "First, lets install the necessary dependencies to run the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "This may take a few minutes to complete as it will first initialize the runtime and then install all the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AuXsoJDZPMs"
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install langchain_google_vertexai\n",
    "!pip install langchain-openai\n",
    "!pip install google-cloud-aiplatform\n",
    "!pip install google-cloud-tasks\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4eVYn8frc5i"
   },
   "source": [
    "## Initialize notebook variables\n",
    "\n",
    "You can fetch all the variables from your lab instance\n",
    "\n",
    "* **PROJECT_ID**: The default GCP project provisioned\n",
    "* **LOCATION**: The default GCP Region where the project is provisioned.\n",
    "* **APIGEE_HOSTNAME**:  The hostname of the Apigee instance provisioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kj-10KmnZSVe"
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"\"  # @param {type:\"string\"}\n",
    "APIGEE_HOSTNAME = \"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Logging with Apigee\n",
    "\n",
    "Logging both prompts and candidate responses from large language models (LLMs) allows for detailed analysis and improvement of the model's performance over time. By examining past interactions, AI practitioners can identify patterns leading to refinements in the training data or model architecture. Furthermore, by examining the prompts security teams can detect malicious intent, such as attempts to extract sensitive information or generate harmful content.\n",
    "\n",
    "Additionally, logging the generated candidates provides insights into the LLM's behavior and helps identify any biases or vulnerabilities in the model itself. This information can then be used to improve security measures, fine-tune the model, and mitigate potential risks associated with LLM usage.\n",
    "\n",
    "![image](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-logging/images/llm-logging.png?raw=1)\n",
    "\n",
    "\n",
    "### Benefits of Logging with Apigee and Google Cloud Logging\n",
    "\n",
    "* **Seamless logging**: Effortlessly capture prompts, candidate responses, and metadata without complex coding.\n",
    "* **Scalable and secure**: Leverage Google Cloud's infrastructure for reliable and secure log management.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Prompt request is receved by an Apigee Proxy.\n",
    "2. Apigee extracts prompt and candidate responses.\n",
    "3. Apigee logs prompt and candidate responses to Cloud Logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S478piq-SGLc"
   },
   "source": [
    "### Test Sample\n",
    "\n",
    "Apigee allows you to seamlessly send logs to Cloud Logging using native integration with the [Message Logging](https://cloud.google.com/apigee/docs/api-platform/reference/policies/message-logging-policy#cloudloggingelement) policy. This sample also includes a message chunking solution that allows logging very long messages (ex. 1M tokens supported by Gemini) and connecting them together using a unique message identifier.\n",
    "\n",
    "With the following cell you'll be able to invoke an LLM and both prompt and candidate resposne will be logged in Cloud Logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87i610BzZns_"
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "if not LOCATION or LOCATION == \"\":\n",
    "    raise ValueError(\"Please set your LOCATION\")\n",
    "if not APIGEE_HOSTNAME or APIGEE_HOSTNAME == \"\":\n",
    "    raise ValueError(\"Please set your APIGEE_HOSTNAME\")\n",
    "\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-logging\"\n",
    "\n",
    "# Initialize Langchain\n",
    "model = VertexAI(\n",
    "      project=PROJECT_ID,\n",
    "      location=LOCATION,\n",
    "      api_endpoint=API_ENDPOINT,\n",
    "      api_transport=\"rest\",\n",
    "      streaming=True,\n",
    "      model_name=\"gemini-1.5-pro\")\n",
    "\n",
    "prompts = [\"Provide an explanation about the benefits of using sunscreen. Make sure to make it as long as a novel.\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "  print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjW8UBEnZg9U"
   },
   "source": [
    "### Explore and Analyze Logs with Cloud Logging\n",
    "\n",
    "1. Navigate to the Cloud Logging Explorer by [right clicking here and opening in a new tab](https://console.cloud.google.com/logs/query?_ga=2.194228271.307340908.1727018794-898542846.1726863667).\n",
    "\n",
    "2. Set the query filter. Make sure to replace the `PROJECT_ID` with the Apigee project ID:\n",
    "\n",
    "  ```\n",
    "  logName=\"projects/PROJECT_ID/logs/apigee\"\n",
    "  ```\n",
    "3. Run the query and explore the logs. See example below:\n",
    "\n",
    "  ![image](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-logging/images/logs-explorer.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully deployed the llm-logging proxy and tested the ability to log the request and responses from subsequent LLM's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Routing with Apigee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a sample Apigee proxy to demonstrate the routing capabilities of Apigee across different LLM providers. In this sample we will use Google VertexAI, Mistral and HuggingFace as the LLM providers\n",
    "- The framework will easily help onboarding other providers using configurations\n",
    "\n",
    "![architecture](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-routing/images/arch.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "This lab requires you to have a HuggingFace Access Token and Mistral AI API Key. The provisioned lab instance **does not** include both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To create a HuggingFace Access Token:\n",
    "- You wll need to create an account in [HuggingFace](https://huggingface.co)\n",
    "- Go to settings, click `Access Tokens` and then click the \"Create new token\" button\n",
    "- Choose `Read` for token type, provide a name and then hit \"Create token\"\n",
    "- Copy the token below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Mistral AI API Key:\n",
    "- You wll need to create an account in [Mistral AI](https://mistral.ai )\n",
    "- Click `API Keys` and then click the \"Create new key\" button\n",
    "- Provide a name and then hit \"Create key\"\n",
    "- Copy the key below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRALAI_KEY=\"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Routing with Apigee:\n",
    "\n",
    "* **Configuration Driven Routing**: All the routing logic are driven through configuration which makes onboarding very easy\n",
    "* **Security**: Irrespective of the model and providers, Apigee will secure the endpoints\n",
    "* **Consistency**: Apigee can offer that layer of consistency to work with any LLM SDKs that are being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update HuggingFace and Mistral AI credentials in Apigee KVM Store\n",
    "\n",
    "We need to update the HuggingFace and Mistral AI credentials in the Apigee KVM store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "import json\n",
    "import requests\n",
    "\n",
    "if not HUGGINGFACE_TOKEN or HUGGINGFACE_TOKEN == \"\":\n",
    "    raise ValueError(\"Please set your HUGGINGFACE_TOKEN\")\n",
    "if not MISTRALAI_KEY or MISTRALAI_KEY == \"\":\n",
    "    raise ValueError(\"Please set your MISTRALAI_KEY\")\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
    "\n",
    "credentials, project_id = default(scopes=SCOPES, quota_project_id=PROJECT_ID)\n",
    "credentials.refresh(Request())\n",
    "access_token = credentials.token\n",
    "\n",
    "url = 'https://apigee.googleapis.com/v1/organizations/'+PROJECT_ID+'/environments/eval/keyvaluemaps/llm-routing-v1-modelprovider-config/entries'\n",
    "headers = {'Authorization': 'Bearer '+access_token, 'Content-type': 'application/json'}\n",
    "\n",
    "entry = 'huggingface__token'\n",
    "resp = requests.put(url+\"/\"+entry, headers = headers, data=json.dumps({\"name\": entry,\"value\": HUGGINGFACE_TOKEN}))\n",
    "if resp.status_code == 200:\n",
    "    print(\"HuggingFace Access Token updated successfully\")\n",
    "else:\n",
    "    print (resp.text)\n",
    "\n",
    "entry = 'mistral__token'\n",
    "resp = requests.put(url+\"/\"+entry, headers = headers, data=json.dumps({\"name\": entry,\"value\": MISTRALAI_KEY}))\n",
    "if resp.status_code == 200:\n",
    "    print(\"Mistral AI API Key updated successfully\")\n",
    "else:\n",
    "    print (resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample\n",
    "\n",
    "We will need an API Key that is already provisioned in your lab instance. Go to Apigee in your GCP console. In the left hand menu, select `Apps` to see the list of Apps. Click the `llm-routing-app` app and copy the `Key` from the `Credentials` section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTING_SAMPLE_APIKEY=\"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "if not LOCATION or LOCATION == \"\":\n",
    "    raise ValueError(\"Please set your LOCATION\")\n",
    "if not APIGEE_HOSTNAME or APIGEE_HOSTNAME == \"\":\n",
    "    raise ValueError(\"Please set your APIGEE_HOSTNAME\")\n",
    "if not ROUTING_SAMPLE_APIKEY or ROUTING_SAMPLE_APIKEY == \"\":\n",
    "    raise ValueError(\"Please set your ROUTING_SAMPLE_APIKEY\")\n",
    "\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-routing/\"\n",
    "\n",
    "PROMPT=\"Suggest name for a flower shop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select an LLM Provider\n",
    "\n",
    "Select a provider from the dropdown. This will automatically set the model name used by the SDKs\n",
    "\n",
    "Try picking different providers from the dropdown above. You will see that the same SDK is able to call the Apigee endpoint serving responses from different providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from google.colab import auth\n",
    "\n",
    "llm_provider = \"select\" # @param [\"select\",\"google\", \"huggingface\", \"mistral\"]\n",
    "\n",
    "if llm_provider == \"google\":\n",
    "    model = \"google/gemini-1.5-flash\"\n",
    "elif llm_provider == \"mistral\":\n",
    "    model = \"open-mistral-nemo\"\n",
    "elif llm_provider == \"huggingface\":\n",
    "    model = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid LLM provider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = ROUTING_SAMPLE_APIKEY\n",
    "openai.base_url = API_ENDPOINT\n",
    "openai.default_headers = {\"x-apikey\": ROUTING_SAMPLE_APIKEY, \"x-llm-provider\": llm_provider}\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": PROMPT\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(f\"Using the OpenAI SDK, fetching the response from \\\"{model}\\\" provided by \\\"{llm_provider}\\\"\")\n",
    "print(\"\\n\")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Langchain SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=model,\n",
    "    api_key=ROUTING_SAMPLE_APIKEY,\n",
    "    base_url=API_ENDPOINT,\n",
    "    default_headers = {\"x-apikey\": ROUTING_SAMPLE_APIKEY, \"x-llm-provider\": llm_provider}\n",
    ")\n",
    "messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": PROMPT\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "print(f\"Using the Langchain SDK, fetching the response from \\\"{model}\\\" provided by \\\"{llm_provider}\\\"\")\n",
    "print(\"\\n\")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully deployed the routing proxy and tested the ability to route calls to different LLM providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Semantic-Caching with Apigee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample performs a cache lookup of responses on Apigee's Cache layer and Vector Search as an embeddings database. It operates by comparing the vector proximity of the prompt to prior requests and using a configurable similarity score threshold.\n",
    "\n",
    "![architecture](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-semantic-cache/images/arch-1.png?raw=1)\n",
    "\n",
    "### Benefits of a Semantic Cache Layer with Apigee:\n",
    "\n",
    "* **Reduced Response Times**: The cache layer significantly reduces response times for repeated queries, as Apigee efficiently stores and retrieves frequently accessed data.\n",
    "* **Improved Efficiency**: By leveraging the caching capabilities of Apigee, unnecessary calls to the underlying model will be minimized, leading to optimized LLM costs.\n",
    "* **Scalability**: The Apigee Cache Layer is managed and distributed, enhancing platform scalability without operational overhead.\n",
    "\n",
    "\n",
    "### About Vertex AI Vector Search API and Embeddings API\n",
    "\n",
    "[**Vertex AI Vector Search**](https://cloud.google.com/vertex-ai/docs/vector-search/overview) enables real-time, fast retrieval of embeddings, which powers a wide range of next-gen user experiences. It provices state-of-the-art embeddigs similarity search ([**ScaNN**](https://research.google/blog/announcing-scann-efficient-vector-similarity-search/)) that is foundational to Google services like Search, Play, and Youtube. It is a key enabler for Search applications and RAG. Vertex AI Vector Search offers speed, scale, quality and cost advantages over alternatives. It also has differentiated value added capabilities including incremental indexing, numerical and tag-based filtering, ensuring diversity of results, and auto-scaling.\n",
    "\n",
    "[**Vertex AI Embeddings API**](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) provides a powerful way to represent text, images, and videos as numerical vectors. It allows for finding similar content based on meaning, Suggesting relevant items based on user preferences and past interactions, classifying, clustering, and detecting outliers based on semantic relationships. It is a key component of RAG architecture allowing more natural and engaging interactions with chatbots. It supports combining information from different data types for richer insights.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Prompt request is receved by an Apigee Proxy.\n",
    "2. Apigee extracts prompt contents and generates a numerical representation using the Vertex AI Embeddings API\n",
    "3. Apigee performs a semantic similarity search using Vertex AI Vector Search\n",
    "4. If there's a datapoint with a good similarity score, then perform a cache lookup using [**Apigee's Cache**](https://cloud.google.com/apigee/docs/api-platform/cache/persistence-tools#caching).\n",
    "5. If there's a cached datapoint, then return the cached LLM response, otherwise populate the Apigee Cache with the LLM respose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample\n",
    "\n",
    "This script measures and visualizes the performance of a semantic cache layer implemented using Apigee.\n",
    "\n",
    "####  Initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "if not LOCATION or LOCATION == \"\":\n",
    "    raise ValueError(\"Please set your LOCATION\")\n",
    "if not APIGEE_HOSTNAME or APIGEE_HOSTNAME == \"\":\n",
    "    raise ValueError(\"Please set your APIGEE_HOSTNAME\")\n",
    "\n",
    "# Define project information\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-semantic-cache\"\n",
    "# Initialize Langchain\n",
    "model = VertexAI(\n",
    "      project=PROJECT_ID,\n",
    "      location=LOCATION,\n",
    "      api_endpoint=API_ENDPOINT,\n",
    "      api_transport=\"rest\",\n",
    "      streaming=True,\n",
    "      model_name=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and analyze semantic cache performance\n",
    "\n",
    "This script measures and visualizes the performance of a semantic cache layer implemented using Apigee.\n",
    "\n",
    "It executes a set of prompts multiple times and records the response times for each execution.\n",
    "The script then plots the response times over the executions, highlighting the average response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "exec = 2\n",
    "execs = []\n",
    "prompts = [\"Why is the sky blue?\",\n",
    "           \"What makes the sky blue?\",\n",
    "           \"Why does the sky is blue colored?\",\n",
    "           \"Can you explain why the sky is blue?\",\n",
    "           \"The sky is blue, why is that?\"]\n",
    "for i in range(exec):\n",
    "  for prompt in prompts:\n",
    "    start_time = time.time()\n",
    "    model.invoke(prompt)\n",
    "    response_time = time.time() - start_time\n",
    "    execs.append(response_time)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "df = pd.DataFrame(execs, columns=['Response time'])\n",
    "df['Exec'] = range(1, len(df) + 1)\n",
    "df.plot(kind='line', x='Exec', y='Response time', legend=False)\n",
    "plt.title('Semantic Cache Performance')\n",
    "plt.xlabel('Executions')\n",
    "plt.ylabel('Response Time')\n",
    "plt.xticks(df['Exec'], rotation=0)\n",
    "\n",
    "average = df['Response time'].mean()\n",
    "plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully deployed the semantic cache proxy and tested the ability to cache prompts effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Token-Limits with Apigee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every interaction with an LLM consumes tokens, therefore, LLM token management plays a crutial role in maintaining platform-level control and visility over the consumption of tokens across LLM providers and consumers.\n",
    "\n",
    "Apigee's API Products, when applied to token consumption, allows you to effectively manage token usage by setting limits on the number of tokens consumed per LLM consumer. This policy leverages the token usage metrics provided by an LLM, enabling real-time monitoring and enforcement of limits.\n",
    "\n",
    "![architecture](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-token-limits/images/ai-product.png?raw=1)\n",
    "\n",
    "\n",
    "### Benefits Token Limits with AI Products\n",
    "\n",
    "Creating Product tiers within Apigee allows for differentiated token quotas at each consumer tier. This enables you to:\n",
    "\n",
    "* **Control resource allocation**: Prioritize resources for high-priority consumers by allocating higher token quotas to their tiers. This will also help to manage platform-wide token budgets across multiple LLM providers.\n",
    "* **Tiered AI products**: By utilizing product tiers with granular token quotas, Apigee effectively manages LLM and empowers AI platform teams to manage costs and provide a multi-tenant platform experience.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Prompt request is receved by an Apigee Proxy.\n",
    "2. Apigee identifies the consumer Application and verifies that the AI Product token quota has not been exceeded.\n",
    "3. Apigee extracts token counts and adds them to quota counter.\n",
    "4. Apigee captures token counts as metrics for Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample\n",
    "\n",
    "We will need the API Keys that are already provisioned in your lab instance. Go to Apigee in your GCP console. In the left hand menu, select Apps to see the list of Apps. Click the `ai-consumer-app` app and copy the Keys from the Credentials section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRONZE_APIKEY=\"\" # @param {type:\"string\"}\n",
    "SILVER_APIKEY=\"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test tiered AI products\n",
    "\n",
    "Apigee allows you to create a tiered product strategy with different API access levels (e.g., Bronze, Silver, Gold) to cater to diverse user needs and limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bronze AI Product\n",
    "\n",
    "This product enforces a 2000 token limit every 5 minutes. Initializing using the Bronze API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "if not LOCATION or LOCATION == \"\":\n",
    "    raise ValueError(\"Please set your LOCATION\")\n",
    "if not APIGEE_HOSTNAME or APIGEE_HOSTNAME == \"\":\n",
    "    raise ValueError(\"Please set your APIGEE_HOSTNAME\")\n",
    "if not BRONZE_APIKEY or BRONZE_APIKEY == \"\":\n",
    "    raise ValueError(\"Please set your BRONZE_APIKEY\")\n",
    "\n",
    "# Define project information\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-token-limits\"\n",
    "# Initialize Langchain\n",
    "model = VertexAI(\n",
    "      project=PROJECT_ID,\n",
    "      location=LOCATION,\n",
    "      api_endpoint=API_ENDPOINT,\n",
    "      api_transport=\"rest\",\n",
    "      streaming=True,\n",
    "      model_name=\"gemini-1.5-pro\",\n",
    "      additional_headers={\"x-apikey\": BRONZE_APIKEY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this limit, follow the steps below:\n",
    "\n",
    "1. Start a debug session in the Apigee console on the **llm-token-limits-v1** proxy that was deployed\n",
    "2. Run the 2000 tokens every 5 minutes test. This scenario demonstrates a basic interaction with a language model. The code repeatedly asks a language model the same question, \"Why is the sky blue?\" but phrased in different ways. It's a simple example of how to interact with a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Why is the sky blue?\",\n",
    "           \"What makes the sky blue?\",\n",
    "           \"Why does the sky is blue colored?\",\n",
    "           \"Can you explain why the sky is blue?\",\n",
    "           \"The sky is blue, why is that?\"]\n",
    "\n",
    "def invoke_model(prompt, model_to_invoke=model):\n",
    "  model_to_invoke.invoke(prompt)\n",
    "\n",
    "for prompt in prompts:\n",
    "  print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After running the scenario, the final token count (sum of tokens from prompts and response candidates) shouldn't exceed the Bronze AI Product tokens limit of 2000 tokens every 5 minutes.\n",
    "4. Now lets run the 5000 tokens every 5 minutes test with the same key. In this scenario we ask the model the same question, \"Why is the sky blue?\" but phrased in different ways to make sure the candidate responses are very **extensive (high token count)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Why is the sky blue? Provide a very long and detailed explanation.\",\n",
    "           \"Furnish and exhaustive and long explanation (as long as a scence magazine article) for the phenomenon of the blue sky.\",\n",
    "           \"Can you give me a really in-depth and as long as a book chapter of why the sky is blue?\",\n",
    "           \"Give me a super detailed and very extensive explanation (as long as the yellow pages) of why the sky is blue.\",\n",
    "           \"Can you tell me all about why the sky is blue, and make sure it's longer than a novel?\"]\n",
    "\n",
    "def invoke_model(prompt, model_to_invoke=model):\n",
    "  model_to_invoke.invoke(prompt)\n",
    "\n",
    "for prompt in prompts:\n",
    "  print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. After running the scenario, the final token count (sum of tokens from prompts and response candidates) **should exceed** the Bronze AI Product tokens limit of 2000 tokens every 5 minutes. Should expect `HTTP 429` error messages in the notebook and also visible on Apigee's debug session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Silver AI Product\n",
    "\n",
    "This product enforces a 5000 token limit every 5 minutes. Initializing using the Silver API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "if not PROJECT_ID or PROJECT_ID == \"\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "if not LOCATION or LOCATION == \"\":\n",
    "    raise ValueError(\"Please set your LOCATION\")\n",
    "if not APIGEE_HOSTNAME or APIGEE_HOSTNAME == \"\":\n",
    "    raise ValueError(\"Please set your APIGEE_HOSTNAME\")\n",
    "if not SILVER_APIKEY or SILVER_APIKEY == \"\":\n",
    "    raise ValueError(\"Please set your SILVER_APIKEY\")\n",
    "\n",
    "# Define project information\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-token-limits\"\n",
    "# Initialize Langchain\n",
    "model = VertexAI(\n",
    "      project=PROJECT_ID,\n",
    "      location=LOCATION,\n",
    "      api_endpoint=API_ENDPOINT,\n",
    "      api_transport=\"rest\",\n",
    "      streaming=True,\n",
    "      model_name=\"gemini-1.5-pro\",\n",
    "      additional_headers={\"x-apikey\": SILVER_APIKEY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this limit, follow the steps below:\n",
    "\n",
    "1. Start a debug session in the Apigee console on the **llm-token-limits-v1** proxy that was deployed\n",
    "2. Run the 2000 tokens every 5 minutes test. This scenario demonstrates a basic interaction with a language model. The code repeatedly asks a language model the same question, \"Why is the sky blue?\" but phrased in different ways. It's a simple example of how to interact with a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Why is the sky blue?\",\n",
    "           \"What makes the sky blue?\",\n",
    "           \"Why does the sky is blue colored?\",\n",
    "           \"Can you explain why the sky is blue?\",\n",
    "           \"The sky is blue, why is that?\"]\n",
    "\n",
    "def invoke_model(prompt, model_to_invoke=model):\n",
    "  model_to_invoke.invoke(prompt)\n",
    "\n",
    "for prompt in prompts:\n",
    "  print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After running the scenario, the final token count (sum of tokens from prompts and response candidates) shouldn't exceed the Silver AI Product tokens limit of 5000 tokens every 5 minutes.\n",
    "4. Now lets run the 5000 tokens every 5 minutes test with the same key. In this scenario we ask the model the same question, \"Why is the sky blue?\" but phrased in different ways to make sure the candidate responses are very **extensive (high token count)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Why is the sky blue? Provide a very long and detailed explanation.\",\n",
    "           \"Furnish and exhaustive and long explanation (as long as a scence magazine article) for the phenomenon of the blue sky.\",\n",
    "           \"Can you give me a really in-depth and as long as a book chapter of why the sky is blue?\",\n",
    "           \"Give me a super detailed and very extensive explanation (as long as the yellow pages) of why the sky is blue.\",\n",
    "           \"Can you tell me all about why the sky is blue, and make sure it's longer than a novel?\"]\n",
    "\n",
    "def invoke_model(prompt, model_to_invoke=model):\n",
    "  model_to_invoke.invoke(prompt)\n",
    "\n",
    "for prompt in prompts:\n",
    "  print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. After running the scenario, the final token count (sum of tokens from prompts and response candidates) **should not exceed** the Silver AI Product tokens limit of 5000 tokens every 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens Consumption Analytics\n",
    "\n",
    "This sample also creates a Tokens Consumption analytics dashboard that allows you to:\n",
    "\n",
    "* Understand usage patterns: See how often tokens are being used and by Developer App.\n",
    "* Optimize token management Make informed decisions about token usage and ajust your tiered limits.\n",
    "* Plan for scalability: Forecast future demand and ensure resource availability.\n",
    "\n",
    "To use this dashboard, from the Apigee console navigate to `Custom Reports` > `Tokens Consumption Report`. You'll be able to drill down into token metrics that represent consumption by Developer Apps and Products. See sample below:\n",
    "\n",
    "![image](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-token-limits/images/token-counts.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully deployed the llm-token-limits proxy and tested the ability to control the access to the LLM workloads using tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LLM-Circuit-Breaking with Apigee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circuit breaking with Apigee offers significant benefits for serving Large Language Models (LLMs) in Retrieval Augmented Generation (RAG) applications, particularly in preventing the dreaded `429` HTTP errors that arise from exceeding LLM endpoint quotas. By placing Apigee between the RAG application and LLM endpoints, users gain a robust mechanism for managing traffic distribution and graceful failure handling.\n",
    "\n",
    "Imagine a scenario where multiple tenants, each with their own LLM endpoints and associated capacity limits, are accessed by a single RAG application. Without circuit breaking, a surge in traffic to a particular tenant's LLM endpoint could trigger a `429` error, disrupting the entire RAG application's functionality. Apigee acts as a traffic cop, monitoring the health of each tenant's endpoint and implementing a circuit-breaking strategy to prevent cascading failures.\n",
    "\n",
    "To further enhance resilience, users can create priority pools, grouping together LLM endpoints with similar capabilities and quota limitations. This allows Apigee to distribute traffic evenly within a pool, effectively aggregating the individual endpoint quotas and ensuring that the combined capacity can handle the load.\n",
    "\n",
    "![image](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-circuit-breaking/images/llm-circuit-breaking.png?raw=1)\n",
    "\n",
    "\n",
    "### Circuit Breaking Benefits\n",
    "\n",
    "1. **Improved fault tolerance**: The multi-pool architecture, coupled with circuit breaking, provides inherent fault tolerance, ensuring that the RAG application remains operational even if one or more LLM endpoints fail or experience outages.\n",
    "2. **Data-driven capacity planning**: Circuit breaking provides valuable insights into endpoint performance, allowing you to monitor and adjust capacity allocations based on actual traffic patterns and usage. This enables informed capacity planning and avoids unnecessary overprovisioning.\n",
    "3. **Multitenancy**: Apigee provides a unified platform for managing and routing traffic to different LLM tenants, simplifying integration and reducing development effort.\n",
    "4. **Centralized monitoring and analytics**: Apigee offers comprehensive monitoring and analytics capabilities, allowing for real-time insights into LLM endpoint performance, quota usage, and failover events. This enables proactive identification and resolution of issues, enhancing operational efficiency.\n",
    "\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Apigee recieves a request and verifies the primary pool status. If it's open, then route the traffic to the primary pool. It it's closed, then route the traffic to the secondary pool.\n",
    "2. If the request to the primary pool fails (`429` or error greater than `399`) then failover to the seconday pool and increase the error count in the circuit breaker.\n",
    "3. Once an max of 2 errors has been detected, then the primary pool is taken out of rotation and all traffic will be sent to the secondary pool.\n",
    "4. The primary pool will be returned back into rotation after a cooldown period of 2 minutes.\n",
    "\n",
    "### Test Sample\n",
    "\n",
    "#### Initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "# Define sample information\n",
    "\n",
    "API_ENDPOINT = \"https://\"+APIGEE_HOSTNAME+\"/v1/samples/llm-circuit-breaking\"\n",
    "TASK_QUEUE = \"ai-queue\"\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
    "GEMINI_SUFFIX = \"/v1/projects/{project}/locations/{location}/publishers/google/models/gemini-1.5-pro:streamGenerateContent\".format(project=PROJECT_ID, location=LOCATION)\n",
    "LLM_REQUEST_URL=API_ENDPOINT + GEMINI_SUFFIX\n",
    "\n",
    "credentials, project_id = default(scopes=SCOPES, quota_project_id=PROJECT_ID)\n",
    "credentials.refresh(Request())\n",
    "access_token = credentials.token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Circuit Breaking\n",
    "\n",
    "The following cell executes a test scenario to exceed the total Gemini quota [gemini-1.5-pro model limits](https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#quotas_by_region_and_model) for a **primary** GCP project. As soon as the project quota is reached, a secondary target will serve traffic without returing `429` errors to the consumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import tasks_v2\n",
    "from google.protobuf import duration_pb2\n",
    "from typing import Dict, Optional\n",
    "import json\n",
    "\n",
    "prompts = [\"Why is the sky blue?\",\n",
    "           \"What makes the sky blue?\",\n",
    "           \"Why does the sky is blue colored?\",\n",
    "           \"Can you explain why the sky is blue?\",\n",
    "           \"The sky is blue, why is that?\",\n",
    "           \"Why is the sky blue?\"]\n",
    "\n",
    "def create_http_task(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    queue: str,\n",
    "    url: str,\n",
    "    json_payload: Dict,\n",
    "    scheduled_seconds_from_now: Optional[int] = None,\n",
    "    task_id: Optional[str] = None,\n",
    "    deadline_in_seconds: Optional[int] = None,\n",
    ") -> tasks_v2.Task:\n",
    "    client = tasks_v2.CloudTasksClient()\n",
    "    task = tasks_v2.Task(\n",
    "        http_request=tasks_v2.HttpRequest(\n",
    "            http_method=tasks_v2.HttpMethod.POST,\n",
    "            url=url,\n",
    "            headers={\"Content-type\": \"application/json\",\n",
    "                     \"Authorization\": f\"Bearer {access_token}\"},\n",
    "            body=json.dumps(json_payload).encode(),\n",
    "        ),\n",
    "        name=(\n",
    "            client.task_path(project, location, queue, task_id)\n",
    "            if task_id is not None\n",
    "            else None\n",
    "        ),\n",
    "    )\n",
    "    duration = duration_pb2.Duration()\n",
    "    duration.FromSeconds(120)\n",
    "    task.dispatch_deadline = duration\n",
    "    return client.create_task(\n",
    "        tasks_v2.CreateTaskRequest(\n",
    "            parent=client.queue_path(project, location, queue),\n",
    "            task=task,\n",
    "        )\n",
    "    )\n",
    "\n",
    "def invoke_model(prompt):\n",
    "  request = {\"contents\":[{\"role\":\"user\",\"parts\":[{\"text\":prompt}]}],\"generationConfig\":{}}\n",
    "  create_http_task(PROJECT_ID, LOCATION, TASK_QUEUE, LLM_REQUEST_URL, request)\n",
    "\n",
    "x = range(15)\n",
    "for n in x:\n",
    "  for prompt in prompts:\n",
    "    invoke_model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze target pool Gemini quotas\n",
    "\n",
    "This sample also creates am LLM Target analytics report that allows you to:\n",
    "\n",
    "* Understand usage patterns: See how often the Gemini quota is being reached.\n",
    "* Optimize token management Make informed decisions about quota usage and ajust pte-allocated quota.\n",
    "* Plan for scalability: Forecast future demand and ensure resource availability.\n",
    "\n",
    "To use this dashboard, from the Apigee console navigate to `Custom Reports` > `LLM Target Report`. You'll be able to drill down into token metrics that represent LLM traffic. \n",
    "\n",
    "**NOTE**: It might take a few mins for the report to show some data\n",
    "\n",
    "See sample below:\n",
    "\n",
    "![image](https://github.com/GoogleCloudPlatform/apigee-samples/blob/main/llm-circuit-breaking/images/circuit-breaking-report.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully deployed the circuit-breaking proxy and tested the ability to switch to a secondary target as a circuit breaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "Congratulations on finishing the workshop! You've now gained valuable skills and hands-on experience in integrating Large Language Models with Apigee. You're well-equipped to build robust, efficient, and innovative AI-powered APIs that can transform your applications and services.\n",
    "\n",
    "We encourage you to continue exploring the exciting world of GenAI and Apigee. Dive deeper into the documentation, experiment with new ideas, and leverage these powerful tools to create cutting-edge solutions.\n",
    "\n",
    "We're excited to see what you build going forward!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
